{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from argparse import ArgumentParser, FileType\n",
    "from importlib import import_module\n",
    "from itertools import count\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from shutil import rmtree\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "import loss\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "EXP_ID =\"190319_triplet-reid_pre-trained_densenet161_veri+small_512/\"\n",
    "TEST_QUERY = False\n",
    "if (TEST_QUERY):\n",
    "    result_folder = \"/home/hthieu/AICityChallenge2019/val_results_temp/\"\n",
    "else:\n",
    "    result_folder = \"/home/hthieu/AICityChallenge2019/val_results/\"\n",
    "\n",
    "EXP_DIR= os.path.join(\"/home/hthieu/AICityChallenge2019/track2_experiments/\", EXP_ID)\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATE\n",
    "query_dataset=\"data/track2_validate_query_v3.csv\"\n",
    "gallery_dataset=\"data/track2_validate_v3.csv\"\n",
    "query_embeddings=os.path.join(EXP_DIR,\"track2_validate_query_embedding.h5\")\n",
    "gallery_embeddings=os.path.join(EXP_DIR,\"track2_validate_embedding.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "query_dataset=\"data/track2_query.csv\"\n",
    "gallery_dataset=\"data/track2_test_v3.csv\"\n",
    "query_embeddings=os.path.join(EXP_DIR,\"track2_query_embedding.h5\")\n",
    "gallery_embeddings=os.path.join(EXP_DIR,\"track2_test_embedding.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSTOM EMBED\n",
    "query_dataset=\"data/track2_test_best_imgs_que.csv\"\n",
    "gallery_dataset=\"data/track2_test_best_imgs.csv\"\n",
    "query_embeddings=os.path.join(EXP_DIR,\"track2_best_imgs_que_embedding.h5\")\n",
    "gallery_embeddings=os.path.join(EXP_DIR,\"track2_best_imgs_embedding.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pids, query_fids, query_views = common.load_dataset(query_dataset, None)\n",
    "gallery_pids, gallery_fids, gallery_views = common.load_dataset(gallery_dataset, None)\n",
    "gallery_views = gallery_views.astype(int)\n",
    "query_views = query_views.astype(int)\n",
    "\n",
    "with h5py.File(query_embeddings, 'r') as f_query:\n",
    "    query_embs = np.array(f_query['emb'])\n",
    "\n",
    "with h5py.File(gallery_embeddings, 'r') as f_gallery:\n",
    "    gallery_embs = np.array(f_gallery['emb'])\n",
    "\n",
    "query_dim = query_embs.shape[1]\n",
    "gallery_dim = gallery_embs.shape[1]\n",
    "if query_dim != gallery_dim:\n",
    "    raise ValueError('Shape mismatch between query ({}) and gallery ({}) '\n",
    "                     'dimension'.format(query_dim, gallery_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'euclidean'\n",
    "batch_pids, batch_fids, batch_embs = tf.data.Dataset.from_tensor_slices(\n",
    "        (query_pids, query_fids, query_embs)\n",
    "    ).batch(batch_size).make_one_shot_iterator().get_next()\n",
    "\n",
    "batch_distances = loss.cdist(batch_embs, gallery_embs, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#Evaluate     :# \n",
    "#################\n",
    "def calculate_ap(fid, pid_match, score):\n",
    "    val_top = np.argsort(score)[-100:][::-1]\n",
    "    ap = average_precision_score(pid_match[val_top], score[val_top])\n",
    "    try:\n",
    "        k = np.where(pid_match[val_top])[0][0]\n",
    "    except:\n",
    "        print(\"Wrong!\")\n",
    "        k = 100\n",
    "        ap = 0.0\n",
    "    if np.isnan(ap):\n",
    "        print()\n",
    "        print(\"WARNING: encountered an AP of NaN!\")\n",
    "        print(\"This usually means a person only appears once.\")\n",
    "        print(\"In this case, it's because of {}.\".format(fid))\n",
    "        print(\"I'm excluding this person from eval and carrying on.\")\n",
    "        print()\n",
    "    return ap, k\n",
    "\n",
    "#################\n",
    "#Save log files:# \n",
    "#################\n",
    "def save_test_img_index(result_folder,ques,aps):\n",
    "    with open(os.path.join(result_folder,\"index.csv\"), \"w\") as fo:\n",
    "        for i in range(len(aps)):\n",
    "            fo.write(\"{},{:.5f}\\n\".format(ques[i],aps[i]))\n",
    "    return\n",
    "\n",
    "def save_predict_results(result_folder,score, pid, pid_match):\n",
    "    #Missing images out of top 100:\n",
    "    val_top = np.argsort(score)[-100:][::-1]\n",
    "    all_imgs = np.argwhere(gallery_pids == pid)\n",
    "    found_imgs = val_top\n",
    "    missing_mask = np.isin(all_imgs,found_imgs, invert=True)\n",
    "    missing_imgs = all_imgs[missing_mask[:,0]][:,0]\n",
    "    with open(os.path.join(result_folder, fids[i].replace('.jpg','.txt')), \"w\") as fo:\n",
    "        for x in found_imgs:\n",
    "            fo.write(\"{:s},{:5f},{},{}\\n\".format(gallery_fids[x].replace('Track2Data',''),\n",
    "                                                 score[x],\n",
    "                                                 pid_match[x],\n",
    "                                                 gallery_views[x]))\n",
    "        for x in missing_imgs:\n",
    "            fo.write(\"{:s},{:5f},{},{}\\n\".format(gallery_fids[x].replace('Track2Data',''),\n",
    "                                                 score[x],\n",
    "                                                 pid_match[x],\n",
    "                                                 gallery_views[x]))\n",
    "        fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Query   area #\n",
    "################\n",
    "def tf_argsort_des(inp_tensor):\n",
    "    return tf.contrib.framework.argsort(inp_tensor,direction='DESCENDING')\n",
    "\n",
    "def tf_count_view_freq(input_views, view_id_max):\n",
    "    tf_cal_row_freq = lambda x : tf.histogram_fixed_width(x, (0,view_id_max),view_id_max + 1)\n",
    "    input_views_freq = tf.map_fn(tf_cal_row_freq,input_views)\n",
    "    input_views_freq_argsorted = tf.map_fn(tf_argsort_des,input_views_freq)\n",
    "    return input_views_freq_argsorted\n",
    "\n",
    "def tf_get_top1_view(batch_distances):\n",
    "    tensor_gal_view = tf.convert_to_tensor(gallery_views, dtype=tf.int32)\n",
    "    gal_view_id_max = np.max(gallery_views)\n",
    "\n",
    "    dis_agr_sorted = tf.contrib.framework.argsort(batch_distances)\n",
    "    dis_agr_sorted_view = tf.gather(tensor_gal_view,dis_agr_sorted)[:,:10]\n",
    "    return tf_count_view_freq(dis_agr_sorted_view, gal_view_id_max)[:,0]\n",
    "\n",
    "def tf_query_imgs(ins):    \n",
    "    ins_distances = loss.cdist(ins, gallery_embs, metric = metric)\n",
    "    return tf.reduce_mean(ins_distances, axis = 0)\n",
    "\n",
    "def tf_query_img_in_same_view(view_mask, tensor_gal_embs):\n",
    "    view_mask = tf.transpose(view_mask)\n",
    "    view_mask = tf.cast(view_mask,tf.bool)\n",
    "    ins = tf.boolean_mask(tensor_gal_embs,view_mask)\n",
    "    return tf.cast(tf_query_imgs(ins),tf.float32)\n",
    "\n",
    "def re_ranking_v2(top_1_view, predict_score): \n",
    "    #Get the top 1 view_id\n",
    "    imp_view = [top_1_view]\n",
    "    #Set images in important views:\n",
    "    for j in range(len(imp_view)):\n",
    "        imp_view_imgs = np.argwhere(gallery_views == imp_view[j])\n",
    "        predict_score[imp_view_imgs] = 1.0\n",
    "    return predict_score\n",
    "\n",
    "def tf_query_extention(distance):\n",
    "    top1_view = tf_get_top1_view(batch_distances)\n",
    "            \n",
    "    tensor_gal_view = tf.convert_to_tensor(gallery_views, dtype=tf.int32)\n",
    "    tensor_gal_embs = tf.convert_to_tensor(gallery_embs)\n",
    "\n",
    "    tmp = tf.ones((tf.size(top1_view),tensor_gal_view.shape[0]), dtype=tf.int32)\n",
    "    tmp = tf.multiply(tmp, top1_view[:,None])\n",
    "    tmp = tf.equal(tmp, tensor_gal_view)\n",
    "    tmp = tf.cast(tmp,tf.float32)\n",
    "    lmb_que_ext = lambda x: tf_query_img_in_same_view(x,tensor_gal_embs)\n",
    "    return tf.map_fn(lmb_que_ext,tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating batch 256-384/1000Wrong!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hthieu/.local/lib/python3.5/site-packages/sklearn/metrics/ranking.py:526: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating batch 384-512/1000Wrong!\n",
      "Calculating batch 512-640/1000Wrong!\n",
      "Wrong!\n",
      "Calculating batch 640-768/1000Wrong!\n",
      "Calculating batch 768-896/1000Wrong!\n",
      "Wrong!\n",
      "Calculating batch 896-1000/1000\n",
      "mAP: 91.10% | top-1: 97.80% top-2: 97.80% | top-5: 97.80% | top-10: 97.80%\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Query area #\n",
    "################\n",
    "aps = []\n",
    "ques = []\n",
    "cmc = np.zeros(len(gallery_pids), dtype=np.int32)\n",
    "gallery_views_id, gallery_views_count = np.unique(gallery_views, return_counts=True)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for start_idx in count(step=batch_size):\n",
    "        try: \n",
    "            top1_view = tf_get_top1_view(batch_distances)\n",
    "            que_ext_re_ranking = tf_query_extention(batch_distances)\n",
    "            top1_views, distances, pids, fids = sess.run([top1_view, que_ext_re_ranking, batch_pids, batch_fids])\n",
    "            print('\\rCalculating batch {}-{}/{}'.format( start_idx, start_idx + len(fids), len(query_fids)), flush=True, end='')\n",
    "    \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print()  # Done!\n",
    "            break\n",
    "        \n",
    "        pids, fids = np.array(pids, '|U'), np.array(fids, '|U')\n",
    "        pid_matches = gallery_pids[None] == pids[:,None]\n",
    "        scores = 1 / (1 + distances)\n",
    "        \n",
    "        for i in range(len(distances)):\n",
    "            fid = fids[i]\n",
    "            pid = pids[i]\n",
    "            pid_match = pid_matches[i,:]\n",
    "            score = scores[i]\n",
    "            top1_view = top1_views[i]\n",
    "            score = re_ranking_v2(top1_view, score)\n",
    "            #Save predict results:\n",
    "            save_predict_results(result_folder,score, pid, pid_match)\n",
    "            \n",
    "            #Calculate AP:\n",
    "            ap, k = calculate_ap(fid, pid_match, score)\n",
    "            cmc[k:] += 1\n",
    "            aps.append(ap)\n",
    "            ques.append(fid)\n",
    "    \n",
    "    # Save index.csv\n",
    "    save_test_img_index(result_folder,ques,aps)  \n",
    "    \n",
    "    # Compute the actual cmc and mAP values\n",
    "    cmc = cmc / len(query_pids)\n",
    "    mean_ap = np.mean(aps)\n",
    "    print('mAP: {:.2%} | top-1: {:.2%} top-2: {:.2%} | top-5: {:.2%} | top-10: {:.2%}'.format(\n",
    "        mean_ap, cmc[0], cmc[1], cmc[4], cmc[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Out of dated #\n",
    "################\n",
    "# Return the top 1 view (same as query image's view in most cases)\n",
    "def get_top_1_view(predict_score):\n",
    "    #Calculate the view_id frequency of the top 10 images with the highest scores\n",
    "    top10 = np.argsort(predict_score)[-10:]\n",
    "    view_id, view_count = np.unique(gallery_views[top10], return_counts=True)\n",
    "    return view_id[np.argsort(view_count)[-1:]][0]\n",
    "    \n",
    "def get_top_100_view_analize():\n",
    "     #Get views info of the top 100 images:\n",
    "    top100 = np.argsort(predict_score)[-100:][::-1]\n",
    "    view100_id, view100_count = np.unique(gallery_views[top100], return_counts=True)\n",
    "    view100_per = np.zeros(len(view100_id))\n",
    "    \n",
    "    with open(os.path.join(result_folder, fids[i].replace('.jpg','_views.txt')), \"w\") as fo:\n",
    "        for j, x in enumerate(np.argsort(view100_count)):\n",
    "            view_count = gallery_views_count[np.where(gallery_views_id == view100_id[x])[0][0]]\n",
    "            view100_per[j] = view100_count[x] / view_count\n",
    "            fo.write(\"{},{},{},{:.5f}\\n\".format(view100_id[x], \n",
    "                                         view100_count[x],\n",
    "                                         view_count,\n",
    "                                         view100_per[j]))\n",
    "        fo.close()\n",
    "    view100_id = view100_id[np.argsort(view100_count)]\n",
    "#                 tmp = view100_id[np.argsort(view100_per)][-2:-1]\n",
    "#                 imp_view = np.concatenate([imp_view,tmp])\n",
    "\n",
    "    imgs_same_view = np.argwhere(gallery_views == query_views[0])\n",
    "    max_diff_id = imgs_same_view[np.argmax(distances[i,imgs_same_view])]\n",
    "    \n",
    "def re_ranking(predict_score): \n",
    "    #Get the top 1 view_id\n",
    "    imp_view = [get_top_1_view(predict_score)]\n",
    "    #Set images in important views:\n",
    "    for j in range(len(imp_view)):\n",
    "        imp_view_imgs = np.argwhere(gallery_views == imp_view[j])\n",
    "        predict_score[imp_view_imgs] = 1.0\n",
    "    return predict_score\n",
    "\n",
    "#####################\n",
    "#My query extention:# \n",
    "#####################\n",
    "def query_imgs(imgId):    \n",
    "    ins = tf.convert_to_tensor(gallery_embs[imgId,:])\n",
    "    ins_distances = loss.cdist(ins, gallery_embs, metric = metric)\n",
    "    return ins_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Out of dated #\n",
    "################\n",
    "aps = []\n",
    "ques = []\n",
    "cmc = np.zeros(len(gallery_pids), dtype=np.int32)\n",
    "gallery_views_id, gallery_views_count = np.unique(gallery_views, return_counts=True)\n",
    "\n",
    "#clear existing results\n",
    "if (os.path.exists(result_folder)):\n",
    "    rmtree(result_folder)\n",
    "os.makedirs(result_folder)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for start_idx in range(1):#count(step=batch_size):\n",
    "        try:\n",
    "            distances, pids, fids = sess.run([batch_distances, batch_pids, batch_fids])\n",
    "            print('\\rCalculating batch {}-{}/{}'.format( start_idx, start_idx + len(fids), len(query_fids)), flush=True, end='')\n",
    "    \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print()  # Done!\n",
    "            break\n",
    "            \n",
    "        pids, fids = np.array(pids, '|U'), np.array(fids, '|U')\n",
    "        pid_matches = gallery_pids[None] == pids[:,None]\n",
    "        scores = 1 / (1 + distances)\n",
    "        \n",
    "        for i in range(1):#range(len(distances)):\n",
    "            fid = fids[i]\n",
    "            pid = pids[i]\n",
    "            pid_match = pid_matches[i,:]\n",
    "            score = scores[i]\n",
    "            #My re-ranking solution:\n",
    "#             score = re_ranking(score)\n",
    "            \n",
    "            #My query extention:\n",
    "            top1_view = get_top_1_view(score)\n",
    "            top1_view_imgs = np.argwhere(gallery_views == top1_view)[:,0]\n",
    "            tmp = sess.run(query_imgs(top1_view_imgs))\n",
    "            tmp = np.average(tmp,axis=0)\n",
    "            tmp = 1 / (1 + tmp)\n",
    "            score = tmp\n",
    "            print(score)\n",
    "            #My re-ranking solution:\n",
    "            score = re_ranking(score)\n",
    "            \n",
    "            #Save predict results:\n",
    "            save_predict_results(result_folder,score, pid, pid_match)\n",
    "            \n",
    "            #Calculate AP:\n",
    "            ap, k = calculate_ap(fid, pid_match, score)\n",
    "            print(ap)\n",
    "            cmc[k:] += 1\n",
    "            aps.append(ap)\n",
    "            ques.append(fid)\n",
    "            \n",
    "            \n",
    "\n",
    "    # Save index.csv\n",
    "    save_test_img_index(result_folder,ques,aps)  \n",
    "    # Compute the actual cmc and mAP values\n",
    "    cmc = cmc / len(query_pids)\n",
    "    mean_ap = np.mean(aps)\n",
    "    print('mAP: {:.2%} | top-1: {:.2%} top-2: {:.2%} | top-5: {:.2%} | top-10: {:.2%}'.format(\n",
    "        mean_ap, cmc[0], cmc[1], cmc[4], cmc[9]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
